{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce62020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1bd390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from ranker import TransformerRanker, CrossEncoderRanker\n",
    "from preprocessor import *\n",
    "from mapper import Mapper\n",
    "from display_products import DisplayProducts\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2d8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/chahaksethi/Desktop/Target/data/1m_recipe/recipe1M_layers/layer1.json\"\n",
    "with open(filepath) as json_data:\n",
    "    recipe = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af345e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input file directory\n",
    "\n",
    "# ip_file_dir = \"../Data/Target Data/\"\n",
    "ip_file_dir = \"/Users/chahaksethi/Desktop/Target/target/target_recipe_project/data/\"\n",
    "# Get grocery product hierarchy information\n",
    "group10 = pd.read_csv(os.path.join(ip_file_dir, \n",
    "                                   'group4_header.csv'),\n",
    "                      sep=',', \n",
    "                      low_memory=False)\n",
    "\n",
    "# Get scraped information for the above products\n",
    "products = pd.read_csv(os.path.join(ip_file_dir,\n",
    "                                    'scraped/products_group4.csv'))\n",
    "\n",
    "# Merge scraped information into the hierarchy table\n",
    "group10 = pd.merge(group10, products, \n",
    "                   how = 'left', on = 'tcin')\n",
    "\n",
    "# Preprocess the table\n",
    "group10 = preprocess_df(group10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf10c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "cross_encoder_name = \"cross-encoder/ms-marco-MiniLM-L-4-v2\"\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0335efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(path):\n",
    "#Load sentences & embeddings from disc\n",
    "    with open(path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_embeddings\n",
    "    \n",
    "def get_tcin_sentence_map(path):\n",
    "#Load sentences & embeddings from disc\n",
    "    with open(path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        df = pd.DataFrame()\n",
    "        df['tcin'] = stored_data['ids']\n",
    "        df['sentence'] = stored_data['sentences']\n",
    "    return df\n",
    "\n",
    "bi_encoder = SentenceTransformer(bi_encoder_name)\n",
    "\n",
    "# Get list of preprocessed product titles\n",
    "product_titles = group10['title'].str.lower().values\n",
    "\n",
    "l1_ranker = TransformerRanker(model=bi_encoder, product_ids=group10['tcin'], max_rank=3)\n",
    "with open('/Users/chahaksethi/Desktop/Target/target/target_recipe_project/data/embeddings/hier_embeddings3.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_ids = stored_data['ids']\n",
    "    stored_sentences = stored_data['sentences']\n",
    "    stored_embeddings = stored_data['embeddings']\n",
    "df = pd.DataFrame()\n",
    "df['id'] = stored_data['ids']\n",
    "df['sentence'] = stored_data['sentences']\n",
    "l1_ranker.load_embeddings(stored_embeddings)\n",
    "\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', max_length=512)\n",
    "l2_ranker = CrossEncoderRanker(bi_model=l1_ranker, \n",
    "                                cross_model=cross_encoder_model, \n",
    "                                tcin_sentence_map=get_tcin_sentence_map('/Users/chahaksethi/Desktop/Target/target/target_recipe_project/data/embeddings/hier_embeddings3.pkl'),\n",
    "                                cross_rank=k,\n",
    "                                bi_rank=30)\n",
    "\n",
    "pm = Mapper(group10)\n",
    "dp = DisplayProducts(ranker=l2_ranker, mapper=pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7c95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_indicator_regex = '(pan|skillet|dish|pot|sheet|grate|whisk|griddle|bowl|oven|saucepan|foil|mortar|pestle|pitcher|bag|cup|stick|blender|paper|knife|glass|brush|colander)'\n",
    "method_indicator_regex = '(boil|bake|baking|stir|roast|fry|rinse|drain|sift|beat|fold|chop|slice|saute|grate|grill|cut)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "265155a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_load(s,n):\n",
    "    recipe_instr=[]\n",
    "    for i in range(s,n):\n",
    "        title = recipe[i]['title']\n",
    "        id = recipe[i]['id']\n",
    "        \n",
    "        for lis in recipe[i]['instructions']:\n",
    "            for key, val in lis.items():   \n",
    "                rem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", val)\n",
    "                if rem !='':\n",
    "                    recipe_instr.append(rem)\n",
    "    return ' '.join(recipe_instr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680158cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tools(instruction_words):\n",
    "    \"\"\"\n",
    "    looks for any and all cooking tools apparent in the instruction text by using the tool_indicator_regex\n",
    "    variable\n",
    "    \"\"\"\n",
    "    cooking_tools = []\n",
    "    for word in instruction_words:\n",
    "        if re.search(tool_indicator_regex, word, flags=re.I):\n",
    "#             print(word)\n",
    "            cooking_tools.append(word)\n",
    "        wordset = set(cooking_tools)\n",
    "    return [item for item in wordset if item.istitle() or item.title() not in wordset]\n",
    "\n",
    "def find_methods(instruction_words):\n",
    "        \"\"\"\n",
    "        looks for any and all cooking methods apparent in the instruction text by using the method_indicator_regex\n",
    "        variable\n",
    "        \"\"\"\n",
    "        cooking_methods = []\n",
    "        for word in instruction_words:\n",
    "            if re.search(method_indicator_regex, word, flags=re.I):\n",
    "                cooking_methods.append(word)\n",
    "            if re.search('preheat', word, re.I):\n",
    "                cooking_methods.append('bake')\n",
    "\n",
    "        wordset = set(cooking_methods)\n",
    "        return [item for item in wordset if item.istitle() or item.title() not in wordset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d37633",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_words = word_tokenize(recipe_load(11000,11001))\n",
    "cooking_tools = find_tools(instruction_words)\n",
    "cooking_methods = find_methods(instruction_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87766e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dish', 'oven', 'bowl', 'colander', 'pot', 'oven-proof']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooking_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e14a809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chop', 'drained', 'boiling', 'Drain', 'Bake']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooking_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ed08603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "class TransformerRanker:\n",
    "    def __init__(self, model, product_ids, max_rank=100,  clf=None):\n",
    "        self.model = model\n",
    "        self.max_rank = max_rank\n",
    "        self.product_ids = product_ids\n",
    "        self.clf = clf\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        self.embeddings = self.model.encode(documents, \n",
    "                                            convert_to_tensor=True)\n",
    "        \n",
    "    def load_embeddings(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def get_scores_ingredient(self, ingredient, max_rank=None):\n",
    "        if not max_rank:\n",
    "            max_rank=self.max_rank\n",
    "\n",
    "        ingredient_embedding = self.model.encode(ingredient, convert_to_tensor=True)\n",
    "        scores = util.pytorch_cos_sim(ingredient_embedding, self.embeddings)[0]\n",
    "        product_scores = dict(zip(self.product_ids, scores.numpy()))\n",
    "        product_scores = sorted(product_scores.items(), \n",
    "                                key = lambda x: x[1], \n",
    "                                reverse=True)[0:100]\n",
    "        if self.clf:\n",
    "            tcin_list =  [product_score[0] for product_score in product_scores]\n",
    "            tcin_list = self.clf.filter_by_class(ingredient, tcin_list)\n",
    "            product_scores = [product_score for product_score in product_scores if product_score[0] in tcin_list]\n",
    "        return product_scores[0:max_rank]\n",
    "        \n",
    "    def get_scores_recipe(self, ingredient_list, max_rank=None):\n",
    "        recipe_scores = []\n",
    "        for ingredient in ingredient_list:\n",
    "            ingredient_scores = self.get_scores_ingredient(ingredient, max_rank)\n",
    "            recipe_scores.append(ingredient_scores)\n",
    "        return recipe_scores\n",
    "\n",
    "    def rank_products_ingredient(self, ingredient, max_rank=None):\n",
    "        product_scores = self.get_scores_ingredient(ingredient, max_rank)\n",
    "        return [product_score[0] for product_score in product_scores]\n",
    "    \n",
    "    def rank_products_recipe(self, ingredient_list, max_rank=None):\n",
    "        recipe_scores = self.get_scores_recipe(ingredient_list, max_rank)\n",
    "        return [[product_score[0] for product_score in product_scores] \n",
    "                for product_scores in recipe_scores]\n",
    "\n",
    "    # Following code is not required\n",
    "    def get_scores_ingredient_custom(self, ingredient, model, tokenizer):\n",
    "        import torch\n",
    "        #Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "        #Tokenize sentences\n",
    "        encoded_input = tokenizer(ingredient, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        #Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "                model_output = model.encoder(\n",
    "                    input_ids=encoded_input[\"input_ids\"], \n",
    "                    attention_mask=encoded_input[\"attention_mask\"], \n",
    "                    return_dict=True\n",
    "                )\n",
    "        #Perform pooling. In this case, mean pooling\n",
    "        ingredient_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        scores = util.pytorch_cos_sim(ingredient_embedding, self.embeddings)[0]\n",
    "        product_score = dict(zip(self.product_ids, scores.numpy()))\n",
    "        product_score = sorted(product_score.items(), \n",
    "                                key = lambda x: x[1], \n",
    "                                reverse=True)\n",
    "        return product_score[0:self.max_rank]\n",
    "\n",
    "\n",
    "class CrossEncoderRanker(TransformerRanker):\n",
    "    def __init__(self, bi_model, cross_model, tcin_sentence_map, cross_rank=10, \n",
    "                 bi_rank=50):\n",
    "        self.bi_model = bi_model\n",
    "        self.cross_model = cross_model\n",
    "        self.cross_rank = cross_rank\n",
    "        self.bi_rank = bi_rank\n",
    "        self.mapper = tcin_sentence_map\n",
    "\n",
    "    def get_scores_ingredient(self, ingredient, max_rank=None):\n",
    "        if not max_rank:\n",
    "            max_rank = self.cross_rank\n",
    "        if isinstance(ingredient, list):\n",
    "            ingredient = ingredient[0]\n",
    "        tcins = self.bi_model.rank_products_ingredient(ingredient, max_rank=self.bi_rank)\n",
    "        sentences = []\n",
    "        for tcin in tcins:\n",
    "            print(tcin)\n",
    "            print(self.mapper[self.mapper['tcin'] == tcin]['sentence'].values)\n",
    "            sentences.append(self.mapper[self.mapper['tcin'] == tcin]['sentence'].values[0])\n",
    "        pairs = [(ingredient, sentence.lower()) for sentence in sentences]\n",
    "        scores = self.cross_model.predict(pairs)\n",
    "        product_score = dict(zip(tcins, scores))\n",
    "        product_score = sorted(product_score.items(), \n",
    "                                key = lambda x: x[1], \n",
    "                                reverse=True)\n",
    "        return product_score[0:max_rank]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83cadc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81994423, 84146438, 83344262, 84292397, 79310833, 84074794, 84650014, 78298231, 76826798, 79318304, 53623409, 76147615, 84184367, 83586751, 76167584, 83290191, 54314565, 75874459, 76150596, 83609142, 76148848, 51317539, 82293186, 81994389, 78833702, 76071368, 84292380, 84172714, 83046727, 80184757]\n",
      "81994423\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fd/fg4j4jtn11q95kb2m8q10q1c0000gn/T/ipykernel_27022/3914388110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Ranked list of product tcin matches for each ingredient - Returns a list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mranked_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_ranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_products_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcooking__tools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fd/fg4j4jtn11q95kb2m8q10q1c0000gn/T/ipykernel_27022/2975528945.py\u001b[0m in \u001b[0;36mrank_products_recipe\u001b[0;34m(self, ingredient_list, max_rank)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrank_products_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingredient_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mrecipe_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredient_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         return [[product_score[0] for product_score in product_scores] \n\u001b[1;32m     47\u001b[0m                 for product_scores in recipe_scores]\n",
      "\u001b[0;32m/var/folders/fd/fg4j4jtn11q95kb2m8q10q1c0000gn/T/ipykernel_27022/2975528945.py\u001b[0m in \u001b[0;36mget_scores_recipe\u001b[0;34m(self, ingredient_list, max_rank)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrecipe_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mingredient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mingredient_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mingredient_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores_ingredient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mrecipe_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredient_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecipe_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fd/fg4j4jtn11q95kb2m8q10q1c0000gn/T/ipykernel_27022/2975528945.py\u001b[0m in \u001b[0;36mget_scores_ingredient\u001b[0;34m(self, ingredient, max_rank)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tcin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtcin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tcin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtcin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', max_length=512)\n",
    "l2_ranker = CrossEncoderRanker(bi_model=l1_ranker, \n",
    "                                cross_model=cross_encoder_model, \n",
    "                                tcin_sentence_map=get_tcin_sentence_map('/Users/chahaksethi/Desktop/Target/target/target_recipe_project/data/embeddings/hier_embeddings3.pkl'),\n",
    "                                cross_rank=k,\n",
    "                                bi_rank=30)\n",
    "\n",
    "# pm = Mapper(group10)\n",
    "# dp = DisplayProducts(ranker=l2_ranker, mapper=pm)\n",
    "\n",
    "cooking__tools = preprocess(cooking_tools)\n",
    "\n",
    "# Ranked list of product tcin matches for each ingredient - Returns a list of lists \n",
    "ranked_match = l2_ranker.rank_products_recipe(cooking__tools, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5b013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401113bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601b147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc8a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "000c1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = recipe_load(100)\n",
    "instruction_words = word_tokenize(text)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b68701ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_lower = [word.lower().strip() for word in instruction_words]\n",
    "ins_lower = [word for word in ins_lower if len(word) > 2]\n",
    "doc = nlp(' '.join(ins_lower))\n",
    "verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "count = Counter(verbs)\n",
    "count_sort = dict(sorted(count.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "920ea7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': 135,\n",
       " 'stir': 41,\n",
       " 'combine': 38,\n",
       " 'set': 31,\n",
       " 'bake': 30,\n",
       " 'remaining': 28,\n",
       " 'cut': 28,\n",
       " 'mix': 27,\n",
       " 'serve': 27,\n",
       " 'let': 26,\n",
       " 'beat': 26,\n",
       " 'cook': 22,\n",
       " 'preheat': 20,\n",
       " 'put': 20,\n",
       " 'sprinkle': 19,\n",
       " 'serving': 18,\n",
       " 'remove': 18,\n",
       " 'make': 17,\n",
       " 'using': 17,\n",
       " 'baking': 16,\n",
       " 'pour': 16,\n",
       " 'bring': 14,\n",
       " 'stirring': 13,\n",
       " 'browned': 12,\n",
       " 'toss': 11,\n",
       " 'cover': 11,\n",
       " 'place': 11,\n",
       " 'spread': 11,\n",
       " 'done': 11,\n",
       " 'turn': 11,\n",
       " 'use': 10,\n",
       " 'chopped': 10,\n",
       " 'are': 10,\n",
       " 'filling': 10,\n",
       " 'uncovered': 9,\n",
       " 'continue': 9,\n",
       " 'stand': 9,\n",
       " 'allow': 9,\n",
       " 'roll': 9,\n",
       " 'keep': 9,\n",
       " 'melt': 8,\n",
       " 'refrigerate': 8,\n",
       " 'inserted': 8,\n",
       " 'boil': 8,\n",
       " 'squash': 8,\n",
       " 'saute': 8,\n",
       " 'greased': 7,\n",
       " 'cooled': 7,\n",
       " 'prepared': 7,\n",
       " 'combined': 7,\n",
       " 'comes': 7,\n",
       " 'take': 7,\n",
       " 'spoon': 7,\n",
       " 'cooked': 7,\n",
       " 'melted': 6,\n",
       " 'dressing': 6,\n",
       " 'lined': 6,\n",
       " 'fold': 6,\n",
       " 'turning': 6,\n",
       " 'reduce': 6,\n",
       " 'beating': 6,\n",
       " 'return': 6,\n",
       " 'enjoy': 5,\n",
       " 'peel': 5,\n",
       " 'rack': 5,\n",
       " 'transfer': 5,\n",
       " 'desired': 5,\n",
       " 'cooking': 5,\n",
       " 'fill': 5,\n",
       " 'granulated': 5,\n",
       " 'have': 5,\n",
       " 'want': 5,\n",
       " 'whisk': 4,\n",
       " 'brown': 4,\n",
       " 'skillet': 4,\n",
       " 'knead': 4,\n",
       " 'cutting': 4,\n",
       " 'spray': 4,\n",
       " 'top': 4,\n",
       " 'chilled': 4,\n",
       " 'seasoning': 4,\n",
       " 'coated': 4,\n",
       " 'freeze': 4,\n",
       " 'blended': 4,\n",
       " 'flatten': 4,\n",
       " 'pull': 4,\n",
       " 'frying': 4,\n",
       " 'cool': 4,\n",
       " 'floured': 4,\n",
       " 'needed': 4,\n",
       " 'covered': 4,\n",
       " 'simmer': 4,\n",
       " 'sit': 3,\n",
       " 'whisking': 3,\n",
       " 'incorporated': 3,\n",
       " 'increase': 3,\n",
       " 'makes': 3,\n",
       " 'drain': 3,\n",
       " 'thickened': 3,\n",
       " 'toasted': 3,\n",
       " 'sifted': 3,\n",
       " 'grate': 3,\n",
       " 'seal': 3,\n",
       " 'prepare': 3,\n",
       " 'divide': 3,\n",
       " 'creamed': 3,\n",
       " 'rise': 3,\n",
       " 'mashed': 3,\n",
       " 'grated': 3,\n",
       " 'break': 3,\n",
       " 'whipped': 3,\n",
       " 'lay': 3,\n",
       " 'flip': 3,\n",
       " 'check': 3,\n",
       " 'covering': 3,\n",
       " 'press': 3,\n",
       " 'start': 3,\n",
       " 'onions': 3,\n",
       " 'fry': 3,\n",
       " 'discard': 3,\n",
       " 'sauce': 3,\n",
       " 'leave': 3,\n",
       " 'making': 3,\n",
       " 'starting': 3,\n",
       " 'arrange': 3,\n",
       " 'heated': 3,\n",
       " 'sugar': 3,\n",
       " 'grease': 3,\n",
       " 'heat': 3,\n",
       " 'scrape': 2,\n",
       " 'bottomed': 2,\n",
       " 'minutes': 2,\n",
       " 'handle': 2,\n",
       " 'absorbed': 2,\n",
       " 'according': 2,\n",
       " 'like': 2,\n",
       " 'reserved': 2,\n",
       " 'buttered': 2,\n",
       " 'chop': 2,\n",
       " 'wish': 2,\n",
       " 'sealed': 2,\n",
       " 'give': 2,\n",
       " 'creating': 2,\n",
       " 'arranging': 2,\n",
       " 'crush': 2,\n",
       " 'crushed': 2,\n",
       " 'starts': 2,\n",
       " 'dried': 2,\n",
       " 'marinate': 2,\n",
       " 'adding': 2,\n",
       " 'diced': 2,\n",
       " 'need': 2,\n",
       " 'blender': 2,\n",
       " 'coconut': 2,\n",
       " 'salted': 2,\n",
       " 'soak': 2,\n",
       " 'folded': 2,\n",
       " 'made': 2,\n",
       " 'sliced': 2,\n",
       " 'minced': 2,\n",
       " 'mixing': 2,\n",
       " 'measure': 2,\n",
       " 'shredded': 2,\n",
       " 'think': 2,\n",
       " 'left': 2,\n",
       " 'ends': 2,\n",
       " 'scatter': 2,\n",
       " 'feel': 2,\n",
       " 'scoop': 2,\n",
       " 'including': 2,\n",
       " 'split': 2,\n",
       " 'crack': 2,\n",
       " 'follow': 2,\n",
       " 'salt': 2,\n",
       " 'store': 2,\n",
       " 'repeat': 2,\n",
       " 'used': 2,\n",
       " 'preheated': 2,\n",
       " 'eat': 2,\n",
       " 'mixed': 2,\n",
       " 'curry': 2,\n",
       " 'beaten': 2,\n",
       " 'arranged': 2,\n",
       " 'going': 2,\n",
       " 'roasting': 2,\n",
       " 'thicken': 2,\n",
       " 'oiled': 2,\n",
       " 'come': 2,\n",
       " 'topping': 2,\n",
       " 'removing': 2,\n",
       " 'ungreased': 2,\n",
       " 'taste': 2,\n",
       " 'almonds': 2,\n",
       " 'topped': 2,\n",
       " 'rinse': 1,\n",
       " 'thickens': 1,\n",
       " 'reheats': 1,\n",
       " 'scorch': 1,\n",
       " 'assembled': 1,\n",
       " 'frozen': 1,\n",
       " 'look': 1,\n",
       " 'celery': 1,\n",
       " 'blend': 1,\n",
       " 'salad': 1,\n",
       " 'pouches': 1,\n",
       " 'dissolve': 1,\n",
       " 'garlic': 1,\n",
       " 'cashew': 1,\n",
       " 'flipping': 1,\n",
       " 'layers': 1,\n",
       " 'given': 1,\n",
       " 'veggies': 1,\n",
       " 'create': 1,\n",
       " 'bakes': 1,\n",
       " 'carrots': 1,\n",
       " 'pestle': 1,\n",
       " 'lengthwise': 1,\n",
       " 'shimmers': 1,\n",
       " 'scraping': 1,\n",
       " 'read': 1,\n",
       " 'registers': 1,\n",
       " 'evaporated': 1,\n",
       " 'heats': 1,\n",
       " 'formed': 1,\n",
       " 'change': 1,\n",
       " \"'re\": 1,\n",
       " 'steak': 1,\n",
       " 'puree': 1,\n",
       " 'has': 1,\n",
       " 'having': 1,\n",
       " 'fit': 1,\n",
       " 'extract': 1,\n",
       " 'followed': 1,\n",
       " 'advance': 1,\n",
       " 'stick': 1,\n",
       " 'sear': 1,\n",
       " 'chives': 1,\n",
       " 'cream': 1,\n",
       " 'overcook': 1,\n",
       " 'quartered': 1,\n",
       " 'cepes': 1,\n",
       " 'caramelize': 1,\n",
       " 'suggested': 1,\n",
       " 'hang': 1,\n",
       " 'sterilized': 1,\n",
       " 'chunks': 1,\n",
       " 'pieces': 1,\n",
       " 'sausage': 1,\n",
       " 'flick': 1,\n",
       " 'sounds': 1,\n",
       " 'halve': 1,\n",
       " 'crosswise': 1,\n",
       " 'flush': 1,\n",
       " 'lift': 1,\n",
       " 'tablespoon': 1,\n",
       " 'moist': 1,\n",
       " 'browns': 1,\n",
       " 'filled': 1,\n",
       " 'hued': 1,\n",
       " 'coating': 1,\n",
       " 'standing': 1,\n",
       " 'liners': 1,\n",
       " 'begin': 1,\n",
       " 'burst': 1,\n",
       " 'coats': 1,\n",
       " 'frosting': 1,\n",
       " 'created': 1,\n",
       " 'tested': 1,\n",
       " 'butter': 1,\n",
       " 'kneading': 1,\n",
       " 'smooth': 1,\n",
       " 'begins': 1,\n",
       " 'appears': 1,\n",
       " 'doubles': 1,\n",
       " 'measuring': 1,\n",
       " 'cross': 1,\n",
       " 'splashing': 1,\n",
       " 'warmed': 1,\n",
       " 'eating': 1,\n",
       " 'unroll': 1,\n",
       " 'slits': 1,\n",
       " 'cms': 1,\n",
       " 'side': 1,\n",
       " 'stacking': 1,\n",
       " 'hold': 1,\n",
       " 'pine': 1,\n",
       " 'flesh': 1,\n",
       " 'carve': 1,\n",
       " 'adjust': 1,\n",
       " 'strain': 1,\n",
       " 'develop': 1,\n",
       " 'grill': 1,\n",
       " 'bending': 1,\n",
       " 'form': 1,\n",
       " 'teaspoons': 1,\n",
       " 'firm': 1,\n",
       " 'dough': 1,\n",
       " 'dry': 1,\n",
       " 'separate': 1,\n",
       " 'rolling': 1,\n",
       " 'sugared': 1,\n",
       " 'overlapping': 1,\n",
       " 'drawn': 1,\n",
       " 'overlap': 1,\n",
       " 'moving': 1,\n",
       " 'apply': 1,\n",
       " 'painting': 1,\n",
       " 'potatoes': 1,\n",
       " 'cooling': 1,\n",
       " 'indicate': 1,\n",
       " 'snaps': 1,\n",
       " 'colander': 1,\n",
       " 'dissolved': 1,\n",
       " 'cracked': 1,\n",
       " 'crispened': 1,\n",
       " 'burn': 1,\n",
       " 'pan': 1,\n",
       " 'stands': 1,\n",
       " 'added': 1,\n",
       " 'weep': 1,\n",
       " 'slack': 1,\n",
       " 'suffer': 1,\n",
       " 'required': 1,\n",
       " 'reached': 1,\n",
       " 'batches': 1,\n",
       " 'defrosted': 1,\n",
       " 'know': 1,\n",
       " 'depending': 1,\n",
       " 'bowls': 1,\n",
       " 'slice': 1,\n",
       " 'edges': 1,\n",
       " 'pressing': 1,\n",
       " 'finishing': 1,\n",
       " 'glaze': 1,\n",
       " 'serves': 1,\n",
       " 'sprinkled': 1,\n",
       " 'powdered': 1,\n",
       " 'distributing': 1,\n",
       " 'choose': 1,\n",
       " 'twists': 1,\n",
       " 'pickled': 1,\n",
       " 'season': 1,\n",
       " 'condensed': 1,\n",
       " 'removed': 1,\n",
       " 'pat': 1,\n",
       " 'came': 1,\n",
       " 'forming': 1,\n",
       " 'touches': 1,\n",
       " 'directed': 1,\n",
       " 'fitted': 1,\n",
       " 'refrigerated': 1,\n",
       " 'crispy': 1,\n",
       " 'herbs': 1,\n",
       " 'stabilize': 1,\n",
       " 'note': 1,\n",
       " 'leftover': 1,\n",
       " 'allowing': 1,\n",
       " 'served': 1,\n",
       " 'run': 1,\n",
       " 'cloves': 1,\n",
       " 'corned': 1,\n",
       " 'replace': 1,\n",
       " 'holds': 1,\n",
       " 'microwave': 1,\n",
       " 'tops': 1,\n",
       " 'crabmeat': 1,\n",
       " 'balls': 1,\n",
       " 'rolled': 1,\n",
       " 'join': 1,\n",
       " 'shaped': 1,\n",
       " 'doubled': 1,\n",
       " 'pierce': 1,\n",
       " 'heating': 1,\n",
       " 'resembles': 1,\n",
       " 'sized': 1,\n",
       " 'wrapping': 1,\n",
       " 'storing': 1,\n",
       " 'reserving': 1,\n",
       " 'correct': 1,\n",
       " 'plump': 1,\n",
       " 'water': 1,\n",
       " 'paste': 1,\n",
       " 'popped': 1,\n",
       " 'soaked': 1,\n",
       " 'skewers': 1,\n",
       " 'brushing': 1,\n",
       " 'lower': 1,\n",
       " 'planning': 1,\n",
       " 'remains': 1,\n",
       " 'reads': 1,\n",
       " 'slicing': 1,\n",
       " 'gives': 1,\n",
       " 'waiting': 1,\n",
       " 'wilted': 1,\n",
       " 'liking': 1,\n",
       " 'seems': 1,\n",
       " 'stuffing': 1,\n",
       " 'goes': 1,\n",
       " 'peas': 1,\n",
       " 'get': 1,\n",
       " 'extend': 1,\n",
       " 'saucepan': 1,\n",
       " 'slivered': 1,\n",
       " 'patting': 1,\n",
       " 'pick': 1,\n",
       " 'chopping': 1,\n",
       " 'whisked': 1,\n",
       " 'inflated': 1,\n",
       " 'changed': 1,\n",
       " 'fluted': 1,\n",
       " 'shake': 1,\n",
       " 'torn': 1,\n",
       " 'wash': 1,\n",
       " 'tinned': 1,\n",
       " 'boiled': 1,\n",
       " 'grind': 1,\n",
       " 'notice': 1,\n",
       " 'closed': 1,\n",
       " 'cornflour': 1,\n",
       " 'process': 1,\n",
       " 'hour': 1,\n",
       " 'grilled': 1,\n",
       " 'heaping': 1,\n",
       " 'beginning': 1,\n",
       " 'crumbled': 1,\n",
       " 'fried': 1,\n",
       " 'reaches': 1,\n",
       " 'slices': 1,\n",
       " 'dredge': 1,\n",
       " 'try': 1,\n",
       " 'roast': 1}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_100_ingredients.to_csv('data/top_100_ingredients.csv', index_label = 'ingredients', header=['recipe_counts'])\n",
    "# pd.read_csv('data/top_100_ingredients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0121b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge('NNP' if label else span.root.tag_, span.text, nlp.vocab.strings[label])\n",
    "\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "matcher.add(entity_key='1', label='ARTIST', attrs={}, specs=[[{spacy.attrs.ORTH: 'Rolling'}, {spacy.attrs.ORTH: 'Stones'}]], on_match=merge_phrases)\n",
    "matcher.add(entity_key='2', label='ARTIST', attrs={}, specs=[[{spacy.attrs.ORTH: 'Muse'}]], on_match=merge_phrases)\n",
    "matcher.add(entity_key='3', label='ARTIST', attrs={}, specs=[[{spacy.attrs.ORTH: 'Arctic'}, {spacy.attrs.ORTH: 'Monkeys'}]], on_match=merge_phrases)\n",
    "doc = nlp(u'The Rolling Stones are an English rock band formed in London in 1962. The first settled line-up consisted of Brian Jones, Ian Stewart, Mick Jagger, Keith Richards, Bill Wyman and Charlie Watts')\n",
    "matcher(doc)\n",
    "for ent in doc.ents:\n",
    "  print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = spacy.blank('en')  # create blank Language class\n",
    "print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (x, {\n",
    "        'entities': [(8, 28, 'TOOL'), (74, 89, 'METHOD')]\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
